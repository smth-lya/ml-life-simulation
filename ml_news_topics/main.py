import json
import random
import time
from typing import Dict, List, Tuple

import torch
from colorama import Fore, Style, init
from tqdm import tqdm
from NaiveBayesClassifier import NaiveBayesClassifier
from config.paths import RAW_DATA_PATH, PROCESSED_DATA_PATH
from data.loader import load_articles
from utils.text_processor import process_text
from utils.vocabulary import build_vocab, save_vocab

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è colorama
init(autoreset=True)

def print_header():
    """–í—ã–≤–æ–¥ –∫—Ä–∞—Å–∏–≤–æ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–æ–≥—Ä–∞–º–º—ã."""
    print(Fore.CYAN + "=" * 70)
    print(Fore.YELLOW + "üöÄ –ù–û–í–û–°–¢–ù–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¢–û–† (–ù–ê–ò–í–ù–´–ô –ë–ê–ô–ï–°)".center(70))
    print(Fore.CYAN + "=" * 70 + Style.RESET_ALL)

def print_step(step: str, description: str):
    """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ç–µ–∫—É—â–µ–º —à–∞–≥–µ."""
    print(Fore.GREEN + f"\n[{step}] {description}" + Style.RESET_ALL)
    time.sleep(0.2)

def load_and_split_data(test_ratio: float = 0.3, random_seed: int = 42) -> Tuple[List, List]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ train/test."""
    print_step("1", "–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    articles = load_articles(RAW_DATA_PATH)
    print(Fore.LIGHTBLUE_EX + f"‚úî –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(articles)} —Å—Ç–∞—Ç–µ–π" + Style.RESET_ALL)

    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    random.seed(random_seed)
    random.shuffle(articles)
    split_idx = int(len(articles) * (1 - test_ratio))
    train_articles = articles[:split_idx]
    test_articles = articles[split_idx:]

    print(Fore.LIGHTBLUE_EX +
          f"‚úî –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train_articles)} —Å—Ç–∞—Ç–µ–π\n"
          f"‚úî –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test_articles)} —Å—Ç–∞—Ç–µ–π" +
          Style.RESET_ALL)

    return train_articles, test_articles

def prepare_vocabulary(train_articles: List, vocab_path: str = PROCESSED_DATA_PATH) -> List:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è."""
    print_step("2", "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ª–æ–≤–∞—Ä—è...")

    if vocab_path.exists():
        with open(vocab_path, 'r', encoding='utf-8') as f:
            vocab = json.load(f)
        print(Fore.LIGHTBLUE_EX + f"‚úî –ó–∞–≥—Ä—É–∂–µ–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å ({len(vocab)} —Ç–æ–∫–µ–Ω–æ–≤)" + Style.RESET_ALL)
    else:
        all_tokens = []
        for article in tqdm(train_articles,
                            desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤",
                            unit="—Å—Ç–∞—Ç—å—è",
                            colour='green'):
            text = article.get("headline", "") + " " + article.get("short_description", "")
            tokens = process_text(text)
            all_tokens.extend(tokens)

        vocab = build_vocab(all_tokens)
        save_vocab(vocab, vocab_path)
        print(Fore.LIGHTBLUE_EX + f"‚úî –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å ({len(vocab)} —Ç–æ–∫–µ–Ω–æ–≤)" + Style.RESET_ALL)

    return vocab

def train_model(train_articles: List, vocab: List) -> NaiveBayesClassifier:
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Naive Bayes."""
    print_step("3", "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Naive Bayes...")

    classifier = NaiveBayesClassifier(vocab, alpha=1.0)
    classifier.train(train_articles)

    print(Fore.LIGHTBLUE_EX +
          f"‚úî –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ {len(train_articles)} —Å—Ç–∞—Ç—å—è—Ö\n"
          f"‚úî –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(classifier.classes_)}" +
          Style.RESET_ALL)

    return classifier

def evaluate_model(classifier: NaiveBayesClassifier, test_articles: List) -> Dict:
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö."""
    print_step("4", "–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

    metrics = classifier.evaluate(test_articles)
    print_evaluation_report(metrics)

    return metrics

def print_evaluation_report(metrics: Dict):
    """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ—Ü–µ–Ω–∫–∏."""
    print(Fore.CYAN + "\n" + "=" * 70)
    print(Fore.YELLOW + "üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–¶–ï–ù–ö–ò –ú–û–î–ï–õ–ò".center(70))
    print(Fore.CYAN + "=" * 70)

    # –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    print(Fore.LIGHTGREEN_EX +
          f"\n–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {metrics['accuracy']:.2%}\n" +
          Style.RESET_ALL)

    # –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–∞–±–ª–∏—Ü—ã
    print(Fore.LIGHTWHITE_EX +
          f"{'–ö–∞—Ç–µ–≥–æ—Ä–∏—è':<25} {'Precision':<10} {'Recall':<10} {'F1-score':<10} {'Support':<10}")
    print("-" * 65)

    # –î–∞–Ω–Ω—ã–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    for cat in sorted(metrics['class_metrics'].keys()):
        m = metrics['class_metrics'][cat]
        print(f"{Fore.WHITE}{cat[:24]:<25} "
              f"{Fore.LIGHTBLUE_EX}{m['precision']:<10.2f} "
              f"{m['recall']:<10.2f} "
              f"{m['f1']:<10.2f} "
              f"{Fore.LIGHTMAGENTA_EX}{m['support']:<10}")

def demo_predictions(classifier: NaiveBayesClassifier, texts: List[str] = None):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π –º–æ–¥–µ–ª–∏."""
    print_step("5", "–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏...")

    if not texts:
        texts = [
            "I love apple",
            "I love Apple",
            "Cleaner Was Dead In Belk Bathroom For 4 Days Before Body Found",
            "Stock market reaches all-time high amid economic recovery",
            "New study shows benefits of Mediterranean diet for heart health",
            "President announces new climate change initiative"
        ]

    print(Fore.CYAN + "\n" + "=" * 70)
    print(Fore.YELLOW + "üì∞ –î–ï–ú–û-–ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –ú–û–î–ï–õ–ò".center(70))
    print(Fore.CYAN + "=" * 70)

    for text in texts:
        probs = classifier.predict_proba(text)
        top3 = sorted(probs.items(), key=lambda x: x[1], reverse=True)[:3]

        print(Fore.LIGHTWHITE_EX + f"\n–¢–µ–∫—Å—Ç: {text}")
        print(Fore.LIGHTGREEN_EX + f"‚ñ∂ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –∫–∞—Ç–µ–≥–æ—Ä–∏—è: {top3[0][0]} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {top3[0][1]:.2%})")

        print(Fore.LIGHTBLUE_EX + "\n–¢–æ–ø-3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for cat, prob in top3:
            print(f"  ‚Ä¢ {cat}: {prob:.2%}")

def run_pipeline(test_ratio: float = 0.3, random_seed: int = 42):
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏."""
    try:
        print_header()

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        train_articles, test_articles = load_and_split_data(test_ratio, random_seed)

        # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–ª–æ–≤–∞—Ä—è
        vocab = prepare_vocabulary(train_articles)

        # 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        classifier = train_model(train_articles, vocab)

        # 4. –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        evaluate_model(classifier, test_articles)

        # 5. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã
        demo_predictions(classifier)

    except Exception as e:
        print(Fore.RED + f"\n‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: {str(e)}" + Style.RESET_ALL)
        raise

if __name__ == '__main__':
    print(torch.cuda.is_available())